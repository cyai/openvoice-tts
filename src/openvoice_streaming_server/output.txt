## __init__.py

<code>


</code>

## core/__init__.py

<code>


</code>

## core/libs/__init__.py

<code>

from openvoice_streaming_server.core.libs.model import StreamingBaseSpeakerTTS

__all__ = ['StreamingBaseSpeakerTTS']

</code>

## core/libs/model.py

<code>

import re
import torch

from openvoice.api import BaseSpeakerTTS


class StreamingBaseSpeakerTTS(BaseSpeakerTTS):
    language_marks = {
        "english": "EN",
        "chinese": "ZH",
    }

    async def generate_audio_chunks(self, text, speaker, language='English', speed=1.0):
        mark = self.language_marks.get(language.lower(), None)
        assert mark is not None, f"language {language} is not supported"

        texts = self.split_sentences_into_pieces(text, mark)

        for t in texts:
            t = re.sub(r'([a-z])([A-Z])', r'\1 \2', t)
            t = f'[{mark}]{t}[{mark}]'
            stn_tst = self.get_text(t, self.hps, False)
            device = self.device
            speaker_id = self.hps.speakers[speaker]
            with torch.no_grad():
                x_tst = stn_tst.unsqueeze(0).to(device)
                x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)
                sid = torch.LongTensor([speaker_id]).to(device)
                audio = self.model.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=0.667, noise_scale_w=0.6,
                                         length_scale=1.0 / speed)[0][0, 0].data.cpu().float().numpy()
                yield audio

    async def tts_stream(self, text, speaker, language='English', speed=1.0):
        async for audio_chunk in self.generate_audio_chunks(text, speaker, language, speed):
            yield audio_chunk.tobytes()

</code>

## core/schemas/__init__.py

<code>

from openvoice_streaming_server.core.schemas.synthesize_schema import SynthesisRequest, SynthesisResponse

__all__ = ["SynthesisRequest", "SynthesisResponse"]

</code>

## core/schemas/synthesize_schema.py

<code>

from pydantic import BaseModel
from typing import Optional, Text


class SynthesisRequest(BaseModel):
    text: Text
    speaker: Optional[Text] = 'default'
    language: Optional[Text] = 'english'
    speed: Optional[float] = 1.0


class SynthesisResponse(BaseModel):
    audio_chunk: bytes

</code>

## core/settings.py

<code>

from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    HOST: str = "127.0.0.1"
    PORT: int = 8000
    DEBUG: bool = True
    LOG_LEVEL: str = "info"

    class Config:
        env_file = ".env"

</code>

## main.py

<code>

from fastapi import FastAPI

from openvoice_streaming_server.v1.api import api_router

app = FastAPI()

# Include API routes
app.include_router(api_router, prefix="/v1")

</code>

## v1/__init__.py

<code>


</code>

## v1/api.py

<code>

from fastapi import APIRouter

from openvoice_streaming_server.v1.endpoints import synthesize

router = APIRouter()
router.include_router(synthesize.router)
api_router = APIRouter()
api_router.include_router(router, prefix="/api")

</code>

## v1/endpoints/__init__.py

<code>


</code>

## v1/endpoints/synthesize.py

<code>

import logging
import torch
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
from starlette.websockets import WebSocketState

from openvoice_streaming_server.core.libs import StreamingBaseSpeakerTTS
from openvoice_streaming_server.core.schemas import SynthesisRequest, SynthesisResponse

router = APIRouter()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
stream_handler = logging.StreamHandler()
stream_handler.setFormatter(formatter)
logger.addHandler(stream_handler)


async def send_audio_stream(websocket: WebSocket, audio_stream):
    try:
        async for audio_chunk in audio_stream:
            response = SynthesisResponse(audio_chunk=audio_chunk)
            await websocket.send_bytes(response.audio_chunk)
    except WebSocketDisconnect:
        pass


class WebSocketHandler:
    def __init__(self, tts_model):
        self.model = tts_model
        self.connections = set()

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.connections.add(websocket)

    async def disconnect(self, websocket: WebSocket):
        if websocket.client_state != WebSocketState.DISCONNECTED:
            await websocket.close()
            self.connections.remove(websocket)

    async def handle_websocket(self, websocket: WebSocket):
        await self.connect(websocket)
        try:
            while True:
                data = await websocket.receive_text()
                request = SynthesisRequest.parse_raw(data)
                if request.text == "":
                    await self.disconnect(websocket)
                    return
                text = request.text
                speaker = request.speaker
                language = request.language
                speed = request.speed
                logger.info(f"Received text: {text}, speaker: {speaker}, language: {language}, speed: {speed}")
                audio_stream = self.model.tts_stream(text, speaker, language, speed)
                await send_audio_stream(websocket, audio_stream)
        except WebSocketDisconnect:
            await self.disconnect(websocket)
        except Exception as e:
            logger.error(f"Error during text synthesis: {e}")
            await self.disconnect(websocket)


en_checkpoint_base = "../resources/checkpoints/base_speakers/EN"
device = "cuda:0" if torch.cuda.is_available() else "cpu"

# Load models and resources
model = StreamingBaseSpeakerTTS(f'{en_checkpoint_base}/config.json', device=device)
model.load_ckpt(f'{en_checkpoint_base}/checkpoint.pth')

handler = WebSocketHandler(model)


@router.websocket("/synthesize")
async def synthesize(websocket: WebSocket):
    await handler.handle_websocket(websocket)

</code>

